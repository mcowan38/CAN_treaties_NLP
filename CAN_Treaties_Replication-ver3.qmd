---
title: "Spirling (2012) Treaties Replication - Methods"
author: "Mike Cowan & Patrick Schnurbusch"
format: 
  html:
    embed-resources: true
    df-print: paged
editor: visual
---

# [CAN_kpcastuff01.R]{.underline}

```{r libraries_dependencies, include=FALSE}
# Generate KPCA for Time Trends + PCA, Correlations, and Random Forests
# Load packages:
library(tidyverse)
library(readxl)
library(plotly)
library(tm)
library(kernlab)
library(SnowballC) # for Similarity Matrix calculations
library(randomForest)
library(xgboost)
library(caret)
library(stringr)
library(kableExtra)

# change global options 
knitr::opts_chunk$set(
  echo = TRUE,            # show code by default
  message = FALSE,        # suppress messages
  warning = FALSE,        # suppress warnings
  fig.width = 6,          # default figure width
  fig.height = 4,         # default figure height
  fig.align = 'center',   # center align figures
  dpi = 300               # figure resolution for high quality
)
```

## **Processing Corpus**

-   VCorpus establishes our Treaty Corpus (rather than "Corpus" in Spirling's original code).

-   Alternatively, Corpus defaults to a PCorpus, which employs lazy loading (utilizes memory as needed). This method is unusable (database issues).

```{r read_data}
# VCorpus adjustment:
treatiesDT <-
  VCorpus(
    DirSource("./00_data/treaties_modified_CAN/Douglas Treaties/"),
    readerControl = list(
      reader = readPlain,
      language = "en",
      load = F
    )
  )

treatiesPF <-
  VCorpus(
    DirSource(
      "./00_data/treaties_modified_CAN/Peace and Friendship Treaties/"),
    readerControl = list(
      reader = readPlain,
      language = "en",
      load = F
    )
  )

treatiesRT <-
  VCorpus(
    DirSource("./00_data/treaties_modified_CAN/Robinson Treaties/"),
    readerControl = list(
      reader = readPlain,
      language = "en",
      load = F
    )
  )

treatiesNT <-
  VCorpus(
    DirSource("./00_data/treaties_modified_CAN/The Numbered Treaties/"),
    readerControl = list(
      reader = readPlain,
      language = "en",
      load = F
    )
  )

treatiesTPN <-
  VCorpus(
    DirSource(
      "./00_data/treaties_modified_CAN/Treaties of Peace and Neutrality/"),
    readerControl = list(
      reader = readPlain,
      language = "en",
      load = F
    )
  )

treatiesUCLS <-
  VCorpus(
    DirSource(
      "./00_data/treaties_modified_CAN/Upper Canada Land Surrenders/"),
    readerControl = list(
      reader = readPlain,
      language = "en",
      load = F
    )
  )

treatiesRP <-
  VCorpus(
    DirSource("./00_data/treaties_modified_CAN/Royal Proclamation/"),
    readerControl = list(
      reader = readPlain,
      language = "en",
      load = F
    )
  )

treatiesWT <-
  VCorpus(
    DirSource("./00_data/treaties_modified_CAN/Williams Treaties/"),
    readerControl = list(
      reader = readPlain,
      language = "en",
      load = F
    )
  )
```

-   Here, **FULL_corp** becomes our original, unaltered corpus database for KPCA and search functionality.
-   **CAN_corp** is used for analysis.
    -   **Note:** here is where we can think about removing treaty-sets collectively (e.g., the Douglas Treaties – treatiesDT).

```{r create_text_corpora, include=FALSE}
# seed for reproducibility:
set.seed(38)

# Generate FULL_corp:
FULL_corp <-
  c(
    treatiesDT,
    treatiesPF,
    treatiesRT,
    treatiesNT,
    treatiesRP,
    treatiesTPN,
    treatiesUCLS,
    treatiesWT
  )

FULL_corp

# Verify sample of the unprocessed text:
inspect(FULL_corp[1:5])  # Inspect the first 5 documents
```

### FULL_corp – Minimal Pre-processing (for KPCA & search function)

-   Converting the VCorpus to a character vector allows the string kernel to process the text data directly.

-   Proceed with analysis WITHOUT stems first (for KPCA).

```{r convert_vcorpus}
# Minimal preprocessing to remove non-textual elements and normalize:
FULL_corp <- tm_map(FULL_corp, content_transformer(function(x) {
  x <- gsub("http[s]?://\\S+", "", x) # Remove URLs
  x <-
    iconv(x, "latin1", "ASCII", sub = "") # Normalize encoding if needed
  return(x)
}))
```

### CAN_corp – Pre-processing (for analysis)

-   Convert to lower case, remove punctuation, remove stop words, and stem.

```{r clean_data, include=FALSE, echo=FALSE}
# Convert to lower case
CAN_corp <- tm_map(FULL_corp, content_transformer(tolower))

# Remove punctuation
CAN_corp <- tm_map(CAN_corp, removePunctuation)

# Remove English stop words
CAN_corp <- tm_map(CAN_corp, removeWords, stopwords("en"))

# Stem the words
CAN_corp <- tm_map(CAN_corp, stemDocument)

# Verify sample of the processed text:
inspect(CAN_corp[1:5])
```

## [**String Kernels**]{.underline}

### 1. Minimal Preprocessing

-   **Preserve Character of Text**: For string kernels, it's beneficial to retain the original character of the text as much as possible.

    -   This includes maintaining the original casing, punctuation, and possibly even misspellings or idiosyncrasies in the text.

    -   These elements can contribute to the distinctive substrings (consecutive characters) that the kernel will analyze.

### 2. Avoid Stemming

-   **Retain Word Forms**: Stemming reduces words to their root forms, which can diminish the lexical diversity that a string kernel might exploit to identify subtle textual similarities or differences.

    -   Avoid stemming unless it's necessary for other parts of your analysis where you need normalized word forms.

### 3. Cleaning Specifics

-   **Remove Non-textual Elements**: Clean the text from non-textual elements such as URLs, email addresses, or any metadata that might skew the analysis.

-   **Unicode Normalization**: If working with diverse languages or fonts, normalize text to a consistent Unicode format to avoid discrepancies due to encoding variations.

### 4. Language-Specific Stopwords

-   **Selective Use of Stopwords**: While Spirling might suggest keeping most of the text intact, removing extremely common stopwords could still be beneficial to reduce noise in the data. However, this should be done selectively to avoid losing meaningful textual context.

### 5. Handling Special Characters

-   **Special Characters and Punctuation**: In some cases, especially when analyzing legal or formal texts, punctuation such as commas, semicolons, or quotation marks could carry semantic weight.
    -   Decide based on the content type whether to retain these elements.

### 6. Error Handling

-   **Error and Anomaly Detection**: Check for anomalies or errors in the text that could significantly impact the analysis, such as corrupted text sections or placeholders from text extraction processes, and clean these appropriately.

## [KPCA]{.underline} – 1, 2, & 10 Components

### **Explanation**

-   **Kernel PCA** **(KPCA)** extends the basic idea of Principal Component Analysis (PCA), which is used to reduce the dimensionality of large data sets – by simplifying them into principal components that capture the most important information.

    1.  **Dealing with Non-Linearity**: Traditional PCA works well with linear relationships, but many real-world data sets – including textual data – exhibit non-linear structures.

        -   KPCA uses kernel functions to project your data into a higher-dimensional space where these non-linear relationships can be linearly separated – potentially uncovering patterns PCA can miss.

    2.  **Major Patterns**: KPCA aims to identify the underlying patterns or trends in the treaty texts – such as themes or recurring issues – by transforming the text data into principal components – new variables that represent significant underlying trends in the data.

    3.  **Reduction and Visualization**: KPCA reduces the complexity of the data, making it easier to analyze and visualize.

        -   e.g., You might use it to see which themes are most prevalent across different treaties or to understand how certain concepts or topics cluster together.

    4.  **Analysis**: The kernel trick allows you to handle text data – typically hard to manage with linear methods – by measuring similarities (or distances) based on the treaties' language and content.

## [Similarity Matrix]{.underline}

-   The similarity matrix method got us to the KPCA, but we had over done the cleaning – via stemming, removing punctuation and white space, etc.

    -   **Type**: The `type="string"` parameter specifies that the kernel should treat the input as strings. This is important because it indicates that the kernel will be analyzing text data, rather than numerical data.
    -   **Length**: The `length=5` parameter refers to the length of **substrings** (also called **n-grams** when discussing text) that the kernel considers when comparing two documents.
    -   In simple terms, this means that the kernel is looking at **how many sequences of 5 consecutive characters** **(n-grams of length 5) are common between any two pieces of text.**

### **Code (kernlab)**

```{r get_similarity_matrix, eval = FALSE}
# [Alternative]: Manually compute the Similarity Matrix:

# # Convert to character:
# FULL_corp <- sapply(FULL_corp, as.character)

# # Define the string kernel:
# stringkern <- stringdot(type = "string", length = 5)

# similarity_matrix <- kernelMatrix(stringkern, doc_texts) # ~8 minute run-time
#
# # KPCA - 1 Feature
# PC1 <-
#   kpca(
#     similarity_matrix,
#     kernel = matrix,
#     kpar = list(sigma = 0.1),
#     features = 1,
#     # features = 1 specifies 1 Principal Component
#     th = 1e-4,
#     na.action = na.omit
#   )
# 
# # Output the result:
# save(PC1, file = "./00_scripts/01_CAN/CAN_PC1.rdata")
# 
# # KPCA - 2 Feature
# PC2 <-
#   kpca(
#     similarity_matrix,
#     kernel = matrix,
#     kpar = list(sigma = 0.1),
#     features = 2,
#     # features = 2 specifies 2 Principal Components
#     th = 1e-4,
#     na.action = na.omit
#   )
# 
# # Output the result:
# save(PC2, file = "./00_scripts/01_CAN/CAN_PC2.rdata")
# 
# # Re-do KPCA with with 8 features (for eiganvalues):
# PC10 <-
#   kpca(
#     similarity_matrix,
#     kernel = matrix,
#     kpar = list(sigma = 0.1),
#     features = 8,
#     th = 1e-4,
#     na.action = na.omit
#   )
# 
# # Output the result:
# save(PC10, file = "./00_scripts/01_CAN/CAN_PC10.rdata")
```

```{r include = FALSE}
# # Convert to character:
# FULL_corp <- sapply(FULL_corp, as.character)
# 
# # Define the string kernel:
# stringkern <- stringdot(type = "string", length = 5)
# 
# # Spirling's Method - KPC1:
# KPC1 <- kpca(
#   FULL_corp,
#   kernel = stringkern,
#   kpar = list(sigma = 0.1),
#   features = 1,
#   th = 1e-4,
#   na.action = na.omit
# )
# 
# # Spirling's Method - KPC2:
# KPC2 <- kpca(
#   FULL_corp,
#   kernel = stringkern,
#   kpar = list(sigma = 0.1),
#   features = 2,
#   th = 1e-4,
#   na.action = na.omit
# )
# 
# # Spirling's Method - KPC10:
# KPC10 <- kpca(
#   FULL_corp,
#   kernel = stringkern,
#   kpar = list(sigma = 0.1),
#   features = 10,
#   th = 1e-4,
#   na.action = na.omit
# )

# Save Results:
# save(KPC1, file = "./00_scripts/01_CAN/CAN_KPC1.rdata")
# save(KPC2, file = "./00_scripts/01_CAN/CAN_KPC2.rdata")
# save(KPC10, file = "./00_scripts/01_CAN/CAN_KPC10.rdata")
```

```{r load_kpca_results}
# Load Data:
load("./00_scripts/01_CAN/CAN_KPC1_douglas.rdata")
load("./00_scripts/01_CAN/CAN_KPC2_douglas.rdata")
load("./00_scripts/01_CAN/CAN_KPC10_douglas.rdata")
```

## **Scree Plot by Eigenvalues**

-   Here we're looking for the elbow (arguably occurs at 1 or 2 eigenvalues).

    -   By **eiganvalues**: indicates the amount of variance in the data that is explained by its corresponding principal component.

    -   Higher eigenvalues mean that the principal component captures more variance from the data.

-   We had to change up how many Principal Components were being detected so we could get a graph out of this (here we look at 10).

### **Scree Plot (plotly)**

```{r scree_plot}
eigenvalues <- KPC10@eig

# Create a df for plotting:
scree_data <- data.frame(
  Principal_Component = seq_along(eigenvalues),
  Eigenvalue = eigenvalues
)

# creating a margins list for plotting
margin_list <- list(autoexpand = FALSE,
               l = 100,
               r = 100,
               t = 110)

plot_ly(
  data = scree_data, 
  x = ~Principal_Component, 
  y = ~Eigenvalue, 
  type = 'scatter', 
  mode = 'lines+markers',
  text = ~paste0("Component: ", Principal_Component, "\nEigenvalue: ", round(Eigenvalue, digits = 3)),
  hoverinfo = 'text', 
  marker = list(size = 9, color = 'red'),    # Black markers
  line = list(color = '#c3c9d2', width = 2.5)        # Blue lines
) %>% 
  # Add title and axis labels with clear titles and units
  layout(
    #title = "<b>Scree Plot - KPCA Results</b>\n10 Components",        # Chart title
    title = list(
      text = paste0("<b>Figure X</b><br><i>KPCA Results - 10 Components</i>"),
      x = 0.1, 
      y = -1,
      xanchor = "left", 
      yanchor = "top", 
      font = list(size = 16)
    ),
    xaxis = list(title = "Principal Components",       # X-axis label
                 type = "linear",                      # Linear scale for X-axis
                 tickformat = ",.0f",                  # Thousands separator for readability
                 titlefont = list(size = 14)),         # Axis label font size
    yaxis = list(title = "Eigenvalue",                 # Y-axis label
                 type = "linear",                      # Linear scale for Y-axis
                 titlefont = list(size = 14)),         # Axis label font size
    
    hovermode = "x unified",  # Unified hover mode removes individual marker dots
    # Font styling for consistency with APA or academic standards
    font = list(family = "Arial", size = 12, color = "black"), # Set global font style
    
    # Set Margins 
    margin = list(l = 100, r = 100, t = 150, b = 100),
    # Set light gray grid lines for clarity without clutter
    xaxis = list(showgrid = TRUE, gridcolor = 'lightgray'),
    yaxis = list(showgrid = TRUE, gridcolor = 'lightgray')
  ) %>% 
  config(displaylogo = FALSE)
```

## Stemming Tests

-   Now we investigate the **KPCA Loadings, Correlations,** and **word importance via Random Forest.**

-   The output here is to a **.csv** file that let's us scroll through every word/stem in the document and see how they relate to our Principal Component – whatever it may be.

### Comparing Component Levels by Treaty

```{r compare_components}
# Use document names vector to attach to the scores:
doc_names <- names(KPC1@xmatrix) # Extract document names

# Create a data frame for the PCA scores:
pca_scores_df <- data.frame(
  Document = doc_names,
  PC1_Score = pc1_scores,
  PC2_Score = pc2_scores
)

# Display the data frame:
print(pca_scores_df)

# Output the PCA scores to a CSV file:
write.csv(
  pca_scores_df,
  "./00_documents/visuals/KPCA_scores_comparison.csv",
  row.names = FALSE
)
```

### Correlations – PC1 (.csv)

-   Clarify if detrended stuff needs to come from KPCA or from the TDM PC1.

```{r get_correlations}
# Convert DTM to matrix and get term frequencies:
term_matrix <- as.matrix(dtm_1)
term_frequencies <-
  colSums(term_matrix > 0) # Using binary presence/absence for correlation

# Check if dimensions match:
if (length(pc1_scores) != nrow(term_matrix)) {
  stop("Mismatch in number of documents and PCA scores")
}

# Calculate correlations for each term:
# We use this to compliment Random Forest plot (Table 1, p. 91).
correlations <- sapply(1:ncol(term_matrix), function(i) {
  cor(term_matrix[, i], pc1_scores, use = "complete.obs", method = "pearson")
})

# Correctly handling NA values in correlations (if any):
if (any(is.na(correlations))) {
  correlations[is.na(correlations)] <-
    0  # Optionally handle NA values
}

# Results:
terms <- colnames(term_matrix)
results_df <- data.frame(Term = terms,
                         Frequency = term_frequencies,
                         Correlation = correlations)

# This orders the output in the .csv by from most positive to most negative:
results_df <- results_df[order(-results_df$Correlation), ]

# Display the top results (to view in R):
print(results_df)

# Output results to a .csv file:
write.csv(
  results_df,
  "./00_documents/visuals/correlations_PC1.csv",
  row.names = FALSE
)
```

```{r positive_word_stems_kpc1}
terms_info_aggregated <- read_excel("00_documents/visuals/terms_info_aggregated.xlsx")

# Correlations - KPC1

## * Positive ---- 
kpc1_positive_word_stems <- terms_info_aggregated %>% 
  dplyr::select(12:14) %>% 
  dplyr::rename(Positive_Term = 1, Frequency = 2, Correlation = 3) %>% 
  dplyr::slice(1:39) %>% 
  data.frame() %>% 
  dplyr::mutate(Correlation = as.numeric(as.character(Correlation))) %>% 
  dplyr::arrange(desc(Correlation)) %>% 
  dplyr::mutate(Correlation = round(x = Correlation, digits = 3))

kpc1_positive_word_stems %>% 
  kbl(caption = "<b>Table N</b> <br><i>KPC1 - Positive Word Stems</i>", 
      col.names = c("Positive Term", "Term Frequncy", "Correlation")) %>%
  kable_classic(full_width = F, html_font = "Times New Roman", lightable_options = "striped") %>% 
  scroll_box(width = "500px", height = "500px") %>% 
  footnote(general = "Here is a general comments of the table. ")

## * Negative ---- 

kpc1_negative_word_stems <- terms_info_aggregated %>% 
  dplyr::select(16:18) %>% 
  dplyr::rename(Negative_Term = 1, Frequency = 2, Correlation = 3) %>% 
  dplyr::slice(1:50) %>% 
  data.frame() %>% 
  dplyr::mutate(Correlation = as.numeric(as.character(Correlation))) %>% 
  dplyr::arrange(desc(Correlation)) %>% 
  dplyr::mutate(Correlation = round(x = Correlation, digits = 3))

kpc1_negative_word_stems %>% 
  kbl(caption = "<b>Table N</b> <br><i>KPC1 - Negative Word Stems</i>",
      col.names = c("Negative Term", "Term Frequncy", "Correlation")) %>%
  kable_classic(full_width = F, html_font = "Times New Roman", lightable_options = "striped") %>% 
  scroll_box(width = "500px", height = "500px") %>% 
  footnote(general = "Here is a general comments of the table. ")

```

### Correlations – PC2 (.csv)

```{r compute_pc2}
# Convert DTM to matrix and get term frequencies:
term_matrix <- as.matrix(dtm_1)

# Calculate term frequencies (using binary presence/absence for correlation):
term_frequencies <- colSums(term_matrix > 0)

# Check if dimensions match:
if (length(pc2_scores) != nrow(term_matrix)) {
  stop("Mismatch in number of documents and PCA scores")
}

# Calculate correlations for each term with the 2nd principal component:
correlations_pc2 <- sapply(1:ncol(term_matrix), function(i) {
  cor(term_matrix[, i], pc2_scores, use = "complete.obs", method = "pearson")
})

# Correctly handle NA values in correlations (if any):
if (any(is.na(correlations_pc2))) {
  correlations_pc2[is.na(correlations_pc2)] <-
    0  # Optionally handle NA values
}

# Make a dataframe to view results:
terms <- colnames(term_matrix)
results_df_pc2 <- data.frame(Term = terms,
                             Frequency = term_frequencies,
                             Correlation = correlations_pc2)

# Order the output by absolute correlation value (from high to low):
results_df_pc2 <-
  results_df_pc2[order(-results_df_pc2$Correlation), ]

# Display the top results (to view in R):
print(results_df_pc2)

# Output results to a .csv file:
write.csv(
  results_df_pc2,
  "./00_documents/visuals/correlations_PC2.csv",
  row.names = FALSE
)
```

```{r}

# Correlations - KPC2

## * Positive ---- 
kpc2_pos_word_stems <- terms_info_aggregated %>% 
  dplyr::slice(52:106) %>% 
  dplyr::select(12:14) %>% 
  dplyr::rename(Positive_Term = 1, Frequency = 2, Correlation = 3) %>% 
  data.frame() %>% 
  dplyr::slice(2:55) %>% 
  dplyr::mutate(Correlation = as.numeric(as.character(Correlation))) %>% 
  dplyr::arrange(desc(Correlation)) %>% 
  dplyr::mutate(Correlation = round(x = Correlation, digits = 3))

kpc2_pos_word_stems %>% 
  kbl(caption = "<b>Table N</b> <br><i>KPC2 - Positive Word Stems</i>", 
      col.names = c("Positive Term", "Term Frequncy", "Correlation")) %>%
  kable_classic(full_width = F, html_font = "Times New Roman", lightable_options = "striped") %>% 
  scroll_box(width = "500px", height = "500px") %>% 
  footnote(general = "Here is a general comments of the table. ")

## * Negative ---- 
kpc2_neg_word_stems <- terms_info_aggregated %>% 
  dplyr::slice(52:106) %>% 
  dplyr::select(16:18) %>% 
  dplyr::rename(Positive_Term = 1, Frequency = 2, Correlation = 3) %>% 
  data.frame() %>% 
  dplyr::slice(2:50) %>% 
  dplyr::mutate(Correlation = as.numeric(as.character(Correlation))) %>% 
  dplyr::arrange(desc(Correlation)) %>% 
  dplyr::mutate(Correlation = round(x = Correlation, digits = 3))

kpc2_neg_word_stems %>% 
  kbl(caption = "<b>Table N</b> <br><i>KPC2 - Negative Word Stems</i>", 
      col.names = c("Positive Term", "Term Frequncy", "Correlation")) %>%
  kable_classic(full_width = F, html_font = "Times New Roman", lightable_options = "striped") %>% 
  scroll_box(width = "500px", height = "500px") %>% 
  footnote(general = "Here is a general comments of the table. ")
```

## [Random Forest Algorithm]{.underline} (p. 89)

**Why use both approaches** (1. KPCA & 2. PCA via TDM with Cor & RF)?

-   **Complementarity**: Kernel PCA and traditional PCA serve complementary roles. Kernel PCA can capture nonlinear relationships in high-dimensional data, making it suitable for complex text data structures. However, its results might be more abstract and harder to interpret directly in terms of original features (words). On the other hand, traditional PCA with a TDM provides a more interpretable model, as it directly relates to the terms used across the documents.

-   **Interpretability and Validation**: Using a TDM and traditional PCA might be intended to validate or supplement the insights gained from the kernel PCA. It offers a more straightforward, interpretable view, directly linking principal components to specific words or terms.

-   **Methodological Rigor**: Employing two methodologies can provide a robustness check, ensuring that findings are not artifacts of a particular method but are consistently observable across different analytical approaches.

### Considerations

-   **Objective Alignment**: Ensure that the analysis method aligns with your research objectives. Kernel PCA might be more suitable if the focus is on uncovering complex patterns and relationships, while traditional PCA is better for direct term importance and variability analysis.

-   **Data Preparation**: For both methods, data preparation is crucial. Ensure that the text is appropriately preprocessed (stemming, stop-word removal, etc.) and that the data subsets used in each analysis are correctly aligned.

-   **Model Interpretation**: Be clear about what each model tells you about the data. Kernel PCA results might require more sophisticated interpretation techniques, whereas traditional PCA results can often be directly interpreted by examining the loadings of principal components.

### **Fit & Details**

-   **Individual Tree Predictions**: Each tree in the Random Forest makes predictions based on a subset of the data and a subset of the features (terms). These subsets are randomly selected, ensuring that each tree develops a unique perspective based on different portions of the dataset and different sets of terms.
-   **Error Estimation**: When a tree is grown, it doesn't use all the available features. This allows for something called "out-of-bag" error estimation, where the data not used in training a particular tree (out-of-bag data) are used to test the tree. This process estimates how each feature (term) impacts the prediction error. For each tree, the error increase when a feature is omitted (by permuting the feature values) indicates the importance of that feature.
-   **Aggregation of Predictions**: The Random Forest algorithm aggregates the predictions from all individual trees to form a final prediction. This aggregation is typically a simple majority vote for classification or an average for regression. This step helps to stabilize the predictions by reducing variance without substantially increasing bias.
-   **Feature Importance Accumulation**: Similarly, feature importance – measured by how much the prediction error increases when a feature is left out---are also aggregated across all trees. This aggregation gives a robust estimate of each term's overall importance across the entire model.
-   **Final Interpretation**: The accumulated feature importance values tell us which terms consistently play a significant role in predicting or explaining the principal component scores. Higher importance suggests a term strongly influences the PCA score, providing insights into the themes or patterns that might be driving the variations captured by the PCA.

## [**Principal Component 1**]{.underline}

### **Basic Random Forest Model – PC1:**

```{r rand_forest_pc1, eval = FALSE}
# Fit Random Forest model for 1 Component:
rf_model <-
  randomForest(as.matrix(dtm_1),
               pc1_scores,
               ntree = 500,
               maxnodes = 5,
               importance = TRUE)

# Print the model summary to check performance:
#print(rf_model)

# Extract importance scores:
importance_scores_pc1 <- importance(rf_model)

# Make a data frame of terms and their importance:
importance_df_pc1 <-
  data.frame(Term = colnames(dtm_1), Importance = importance_scores_pc1[, "%IncMSE"])

# Sort the terms by decreasing importance:
importance_df_pc1 <- importance_df_pc1[order(-importance_df_pc1$Importance), ]

# Display the top important terms:
top_terms <- head(importance_df_pc1, 35)

# Optionally, output results to .csv file:
write.csv(
  importance_df_pc1,
  "./00_documents/visuals/term_importance_PC1.csv",
  row.names = FALSE
)
```

```{r import_term_importance_pc1}
importance_df_pc1 <- read.csv("00_documents/visuals/term_importance_PC1.csv")
```

```{r}
importance_df_pc1 %>% 
  dplyr::filter(!is.na(Importance)) %>% 
  slice_max(order_by = Importance, n = 35) %>% 
  `rownames<-`( NULL ) %>% # deletes rownames 
  dplyr::mutate(Importance = round(x = Importance, digits = 3)) %>% 
  kbl(caption = "<b>Table N</b> <br><i>PC1 Term Importance Scores</i>") %>%
  kable_classic(full_width = F, html_font = "Times New Roman", lightable_options = "striped") %>% 
  scroll_box(width = "500px", height = "500px") %>% 
  footnote(general = "Here is a general comments of the table. ")
```

### **XGBoost Model Tuning – PC1:**

```{r xgb_pc1}
# Convert term-document matrix to matrix format
# dtm_matrix <- as.matrix(dtm_1)

# Convert the response variable to a numeric vector
# response_vector <- as.numeric(pc1_scores)

# Create an xgb.DMatrix object
# dtrain <- xgb.DMatrix(data = dtm_matrix, label = response_vector)

# param_grid <- expand.grid(
#   max_depth = c(4, 6, 8),
#   eta = c(0.01, 0.1, 0.3),
#   gamma = c(0, 0.1, 0.2),
#   colsample_bytree = c(0.6, 0.8, 1),
#   min_child_weight = c(1, 3, 5),
#   subsample = c(0.6, 0.8, 1),
#   nrounds = c(100, 200, 500)
# )

# Create cross-validation folds
# set.seed(123)
# cv_folds <- createFolds(response_vector, k = 3, list = TRUE)

# Function to run cross-validation and return RMSE
# cv_rmse <- function(params) {
#   cv <- xgb.cv(
#     params = params,
#     data = dtrain,
#     nrounds = params$nrounds,
#     folds = cv_folds,
#     metrics = "rmse",
#     verbose = FALSE
#   )
#   return(min(cv$evaluation_log$test_rmse_mean))
# }
# best_params <- NULL
# best_rmse <- Inf

# for (i in 1:nrow(param_grid)) {
#   params <- as.list(param_grid[i,])
#   params$objective <- "reg:squarederror"

#   rmse <- cv_rmse(params)

#   if (rmse < best_rmse) {
#     best_rmse <- rmse
#     best_params <- params
#   }

#   print(paste("Completed iteration", i, "with RMSE:", rmse))
# }

# print(best_params)
# print(paste("Best RMSE:", best_rmse))

# final_model <- xgb.train(
#   params = best_params,
#   data = dtrain,
#   nrounds = best_params$nrounds
# )

# Make predictions (using iteration_range)
# preds <- predict(final_model, dtrain, iteration_range = c(1, best_params$nrounds))
```

### **XGBoost Model – PC1:**

```{r xgb, eval = FALSE}
# Convert term-document matrix to matrix format
dtm_matrix_1 <- as.matrix(dtm_1)

# Convert the response variable to a numeric vector
response_vector_1 <- as.numeric(pc1_scores)

# Create an xgb.DMatrix object
dtrain_1 <- xgb.DMatrix(data = dtm_matrix_1, label = response_vector_1)

# Set parameters for the XGBoost model
params_1 <- list(
  objective = "reg:squarederror",
  max_depth = 5,     # Reduced from  to limit tree depth
  eta = 0.1,         # Changed from 0.3 to 0.1
  nthread = 2,
  eval_metric = "rmse",
  alpha = 1,         # L1 regularization term
  lambda = 1         # L2 regularization term
)

xgb_model_1 <- xgb.train(
  params = params_1,
  data = dtrain_1,
  nrounds = 100       # Changed from 2000 to 100
)

# Get feature importance scores
importance_scores_1 <- xgb.importance(feature_names = colnames(dtm_matrix_1), model = xgb_model_1)

# Convert to a data frame
importance_df_1 <- as.data.frame(importance_scores_1)

# Sort the terms by decreasing importance
importance_df_1 <- importance_df_1[order(-importance_df_1$Gain), ]

# Display the top important terms
top_terms_1 <- head(importance_df_1, 35)
print(top_terms_1)

# Optionally, output results to .csv file:
write.csv(
  importance_df_1,
  "./00_documents/visuals/term_importance_xgboost_PC1.csv",
  row.names = FALSE
)
```

```{r import_term_importance_xgboost_pc1}
importance_df_1 <- read.csv(  "./00_documents/visuals/term_importance_xgboost_PC1.csv")
```

```{r}
importance_df_1 %>% 
  dplyr::mutate(across(c(Gain, Cover, Frequency), ~ round(.x, 3))) %>% 
  kbl(caption = "<b>Table N</b> <br><i>PC1 XGB Scores</i>") %>%
  kable_classic(full_width = F, html_font = "Times New Roman", lightable_options = "striped") %>% 
  scroll_box(width = "500px", height = "500px") %>% 
  footnote(general = "Here is a general comments of the table. ")
```

## [**Principal Component 2**]{.underline}

### **Basic Random Forest Model – PC2:**

```{r rand_forest_pc2}
# Fit the Random Forest model using PC2 scores:
rf_model_pc2 <- randomForest(
  as.matrix(dtm_1),
  pc2_scores,
  ntree = 500,
  maxnodes = 5,
  importance = TRUE
)

# Print the model summary to check performance:
print(rf_model_pc2)

# Extract importance scores:
importance_scores_pc2 <- importance(rf_model_pc2)

# Make a data frame of terms and their importance:
importance_df_pc2 <-
  data.frame(Term = colnames(dtm_1), Importance = importance_scores_pc2[, "%IncMSE"])

# Sort the terms by decreasing importance:
importance_df_pc2 <-
  importance_df_pc2[order(-importance_df_pc2$Importance),]

# Display the top important terms:
top_terms_pc2 <- head(importance_df_pc2, 35)
print(top_terms_pc2)

# Optionally, output results to .csv file:
write.csv(
  importance_df_pc2,
  "./00_documents/visuals/term_importance_PC2.csv",
  row.names = FALSE
)
```

### **XGBoost Model Tuning – PC2:**

```{r xgb_tuning_pc2}
# Convert term-document matrix to matrix format
# dtm_matrix <- as.matrix(dtm_1)

# Convert the response variable to a numeric vector
# response_vector <- as.numeric(pc2_scores)

# Create an xgb.DMatrix object
# dtrain <- xgb.DMatrix(data = dtm_matrix, label = response_vector)

# param_grid <- expand.grid(
#   max_depth = c(4, 6, 8),
#   eta = c(0.01, 0.1, 0.3),
#   gamma = c(0, 0.1, 0.2),
#   colsample_bytree = c(0.6, 0.8, 1),
#   min_child_weight = c(1, 3, 5),
#   subsample = c(0.6, 0.8, 1),
#   nrounds = c(100, 200, 500)
# )

# Create cross-validation folds
# set.seed(123)
# cv_folds <- createFolds(response_vector, k = 3, list = TRUE)

# Function to run cross-validation and return RMSE
# cv_rmse <- function(params) {
#   cv <- xgb.cv(
#     params = params,
#     data = dtrain,
#     nrounds = params$nrounds,
#     folds = cv_folds,
#     metrics = "rmse",
#     verbose = FALSE
#   )
#   return(min(cv$evaluation_log$test_rmse_mean))
# }
# best_params <- NULL
# best_rmse <- Inf

# for (i in 1:nrow(param_grid)) {
#   params <- as.list(param_grid[i,])
#   params$objective <- "reg:squarederror"
  
#   rmse <- cv_rmse(params)
  
#   if (rmse < best_rmse) {
#     best_rmse <- rmse
#     best_params <- params
#   }
  
#   print(paste("Completed iteration", i, "with RMSE:", rmse))
# }

# print(best_params)
# print(paste("Best RMSE:", best_rmse))

# final_model <- xgb.train(
#   params = best_params,
#   data = dtrain,
#   nrounds = best_params$nrounds
# )

# Make predictions (using iteration_range)
# preds <- predict(final_model, dtrain, iteration_range = c(1, best_params$nrounds))
# preds
```

### **XGBoost Model – PC2:**

```{r xgb_pc2}
# Convert term-document matrix to matrix format
dtm_matrix_2 <- as.matrix(dtm_1)

# Convert the response variable to a numeric vector
response_vector_2 <- as.numeric(pc2_scores)

# Create an xgb.DMatrix object
dtrain_2 <- xgb.DMatrix(data = dtm_matrix_2, label = response_vector_2)

# Set parameters for the XGBoost model
params_2 <- list(
  objective = "reg:squarederror",
  max_depth = 5,     # Reduced from  to limit tree depth
  eta = 0.1,         # Changed from 0.3 to 0.1
  nthread = 2,
  eval_metric = "rmse",
  alpha = 1,         # L1 regularization term
  lambda = 1         # L2 regularization term
)

# Train the XGBoost model
xgb_model_2 <- xgb.train(
  params = params_2,
  data = dtrain_2,
  nrounds = 100 # Number of boosting rounds
)

# Get feature importance scores
importance_scores_xgb2 <- xgb.importance(feature_names = colnames(dtm_matrix_2), model = xgb_model_2)

# Convert to a data frame
importance_df_xgb_2 <- as.data.frame(importance_scores_xgb2)

# Sort the terms by decreasing importance
importance_df_xgb_2 <- importance_df_xgb_2[order(-importance_df_xgb_2$Gain), ]

# Display the top important terms
top_terms_xgb_2 <- head(importance_df_xgb_2, 35)
print(top_terms_xgb_2)

# Optionally, output results to .csv file:
write.csv(
  importance_df_xgb_2,
  "./00_documents/visuals/term_importance_xgboost_PC2.csv",
  row.names = FALSE
)
```

```{r}
importance_df_xgb_2 %>% 
  dplyr::mutate(across(c(Gain, Cover, Frequency), ~ round(.x, 3))) %>% 
  kbl(caption = "<b>Table N</b> <br><i>PC2 XGB Scores</i>") %>%
  kable_classic(full_width = F, html_font = "Times New Roman", lightable_options = "striped") %>% 
  scroll_box(width = "500px", height = "500px") %>% 
  footnote(general = "Here is a general comments of the table. ")
```

## [**Visualizations**]{.underline}

### **RF – Visualize PC1 Feature Importance:**

```{r brf_visualize_feature_importance_scores}
#| fig-width: 10
#| fig-height: 12
#| 
importance_df_pc1$Term <- factor(importance_df_pc1$Term, levels = unique(importance_df_pc1$Term)[order(importance_df_pc1$Importance, decreasing = F)])

plot_ly(data = importance_df_pc1[1:35,], x = ~Importance, y = ~Term, type = 'bar',
        marker = list(color = 'steelblue'), 
        height = 1000 # explicitly set height
        ) %>% 
  layout(
    title = list(
      text = paste0("<b>Figure X</b><br><i>PC1 Model Results - Top 35 Terms</i>"),
      x = 0.1, 
      y = -1,
      xanchor = "left", 
      yanchor = "top", 
      font = list(size = 16)
    ),
    #xaxis = xaxis, 
    xaxis = list(title = "<b>Increase in Mean Squared Error (%)</b>",       # X-axis label
                 titlefont = list(size = 14),  # Axis label font size
                 showline = TRUE,
                 linecolor = 'rgb(204, 204, 204)',
                 showgrid = FALSE,
                 showticklabels = TRUE,
                 linecolor = 'rgb(204, 204, 204)',
                 linewidth = 2,
                 autotick = FALSE,
                 ticks = 'outside',
                 tickcolor = 'rgb(204, 204, 204)',
                 tickwidth = 2,
                 ticklen = 5,
                 tickfont = list(family = 'Arial', size = 12, color = 'rgb(82, 82, 82)')
                 ),        
    yaxis = list(
      title = "<b>Term</b>",
      ticksuffix = "   ",
      tickfont = list(family = 'Arial', size = 14, color = 'rgb(82, 82, 82)'), # Larger font size for terms
      tickpadding = 10, # Adjust padding as needed to increase space
      categoryorder = "total ascending" # Ensures all factor levels are shown
    ), 
    
    hovermode = "y unified", 
    # Set Margins 
    margin = list(l = 100, r = 100, t = 150, b = 100)) %>% 
  config(displaylogo = FALSE)
```

### **RF – Visualize PC2 Feature Importance:**

```{r visualize_pc2_feature_importance_scores}
#| fig-width: 10
#| fig-height: 12


rownames(top_terms_pc2) <- NULL


top_terms_pc2$Term <- factor( top_terms_pc2$Term, levels = unique( top_terms_pc2$Term)[order( top_terms_pc2$Importance, decreasing = F)])


plot_ly(data =  top_terms_pc2[1:35,], x = ~Importance, y = ~Term, type = 'bar',
        marker = list(color = 'red'), 
        height = 1000 # explicitly set height
) %>% 
  layout(
    title = list(
      text = paste0("<b>Figure X</b><br><i>PC2 RF Model Results - Top 35 Terms</i>"),
      x = 0.1, 
      y = -1,
      xanchor = "left", 
      yanchor = "top", 
      font = list(size = 16)
    ),
    #xaxis = xaxis, 
    xaxis = list(title = "<b>Increase in Mean Squared Error (%)</b>",       # X-axis label
                 titlefont = list(size = 14),  # Axis label font size
                 showline = TRUE,
                 linecolor = 'rgb(204, 204, 204)',
                 showgrid = FALSE,
                 showticklabels = TRUE,
                 linecolor = 'rgb(204, 204, 204)',
                 linewidth = 2,
                 autotick = FALSE,
                 ticks = 'outside',
                 tickcolor = 'rgb(204, 204, 204)',
                 tickwidth = 2,
                 ticklen = 5,
                 tickfont = list(family = 'Arial', size = 12, color = 'rgb(82, 82, 82)')
    ),        
    yaxis = list(
      title = "<b>Term</b>",
      ticksuffix = "   ",
      tickfont = list(family = 'Arial', size = 14, color = 'rgb(82, 82, 82)'), # Larger font size for terms
      tickpadding = 10, # Adjust padding as needed to increase space
      categoryorder = "total ascending" # Ensures all factor levels are shown
    ), 
    
    hovermode = "y unified", 
    # Set Margins 
    margin = list(l = 100, r = 100, t = 150, b = 100)) %>% 
  config(displaylogo = FALSE)

```

### **RF Models vs. Prediction – Correlations:**

```{r brf_models_v_predictions_correlations}
# Calculate correlations:
cor_pc1 <- cor(pc1_scores, predict(rf_model))
cor_pc2 <- cor(pc2_scores, predict(rf_model_pc2))

# View the correlations:
print(cor_pc1)
print(cor_pc2)
```

### **Biplot: KPC1 & KPC2**

```{r biplot_kpc1_v_kpc2}
# Load PCA scores
pca_scores_df <- read_csv("./00_documents/visuals/KPCA_scores_comparison.csv")

# Remove the ".txt" extension from the 'Document' column
pca_scores_df <- pca_scores_df %>%
  mutate(Document = str_remove(Document, "\\.txt$"))

# Load treaty metadata
treaty_metadata <- read_csv("./00_documents/01_CAN_spreadsheets/CAN_universe_cases_douglas.csv")

# Clean and prepare dates
treaty_metadata <- treaty_metadata %>%
  mutate(Date = as.Date(str_replace_all(Date, "/", "-"), format = "%Y-%m-%d"))

# Merge PCA scores with metadata based on the Document identifier
pca_merged_df <- pca_scores_df %>%
  left_join(treaty_metadata, by = c("Document" = "Treaty ID"))

# Update the 'Treaty' column using case_when to replace values
pca_merged_df <- pca_merged_df %>%
  dplyr::mutate(Type = gsub("^The Numbered Treaties.*", "The Numbered Treaties", Type))

# Define your custom color palette as a list of hex codes
custom_colors <- c("#EF476F", "#F78C6B", "#FFD166", "#83D483", "#06D6A0", "#0CB0A9", "#118AB2", "#073B4C") # Add more colors as needed

# Create and display the interactive biplot
biplot <- plot_ly(
  data = pca_merged_df,
  x = ~PC1_Score,
  y = ~PC2_Score,
  type = 'scatter',
  mode = 'markers',
  color = ~Type,
  colors = custom_colors,  # Use custom color palette here
  marker = list(size = 10, opacity = 0.8, line = list(width = 1, color = '#FFFFFF')),
  text = ~paste(
    "Treaty Name:", `Treaty Name`,
    "<br>Type:", Type,
    "<br>Date:", Date,
    "<br>PC1 Score:", round(PC1_Score, 3),
    "<br>PC2 Score:", round(PC2_Score, 3)
  ),
  hoverinfo = 'text'
) %>%
  layout(
    title = list(
      text = paste0(
        "<b>Figure X</b>
        <br><i>Biplot of Canadian-Indigenous Treaties</i>"),
      x = 0.1, 
      y = -1,
      xanchor = "left", yanchor = "top", font = list(size = 16)
    ),
    xaxis = list(title = "<b>Principal Component 1 (KPC1)", titlefont = list(size = 14), tickfont = list(size = 12)),
    yaxis = list(title = "<b>Principal Component 2 (KPC2)", titlefont = list(size = 14), tickfont = list(size = 12)),
    legend = list(title = list(text = "Treaty Type", font = list(size = 14)), font = list(size = 12)),
    margin = list(l = 100, r = 100, t = 150, b = 200)
  ) %>% 
  add_annotations(
    text = "<i>Note. High KPC1 indicates a focus on land transactions, property rights, and financial compensation; 
    low KPC1 emphasizes governance, legal administration, and Indigenous relations.
    High KPC2 reflects formal legal language with references to the British Crown;
    low KPC2 denotes practical and informal language.
    ",
    x = 0.5,  # Centered horizontally
    y = -1,  # Position below the graph
    xref = "paper",
    yref = "paper",
    showarrow = FALSE
  )

biplot
# Save the interactive biplot as an HTML file
saveWidget(biplot, "biplot_canadian_indigenous_treaties.html")
```

### **Principal Components Table:**

```{r principle_components_table}
# Load PCA scores
pca_scores_df <- read_csv("./00_documents/visuals/KPCA_scores_comparison.csv")

# Remove the ".txt" extension from the 'Document' column
pca_scores_df <- pca_scores_df %>%
  mutate(Document = str_remove(Document, "\\.txt$"))

# Load treaty metadata
treaty_metadata <- read_csv("./00_documents/01_CAN_spreadsheets/CAN_universe_cases_douglas.csv")

# Clean and prepare dates
treaty_metadata <- treaty_metadata %>%
  mutate(Date = as.Date(str_replace_all(Date, "/", "-"), format = "%Y-%m-%d"))

# Merge PCA scores with metadata based on the Document identifier
pca_merged_df <- pca_scores_df %>%
  left_join(treaty_metadata, by = c("Document" = "Treaty ID"))

# Summarize KPC1 and KPC2 based on descriptions in the subtitle
pca_merged_df <- pca_merged_df %>%
  mutate(
    KPC1_Summary = if_else(PC1_Score > 0, "Land", "Governance"),
    KPC2_Summary = if_else(PC2_Score > 0, "Formal", "Practical")
  )

# Select the columns: Treaty Name, KPC1_Summary, PC1_Score, KPC2_Summary, and PC2_Score
summary_table <- pca_merged_df %>%
  select(`Treaty Name`, KPC1_Summary, PC1_Score, KPC2_Summary, PC2_Score)

# Display the table
summary_table
```

## [**Relevant Sentences Search Function**]{.underline}

```{r search_terms_in_corpus_function}
# Function to search within each document of the corpus for any occurrence
search_term_in_corpus <- function(FULL_corp, search_term) {
  results <- lapply(FULL_corp, function(doc) {
  # Tokenize document content into sentences
    sentences <-
      unlist(strsplit(as.character(doc), split = "[.!?]"))
      # Search for the term within sentences using a pattern that captures                 derivatives of "punish"
    relevant_sentences <-
      sentences[str_detect(sentences, pattern = paste0(search_term))]
    return(relevant_sentences)
  })
  return(unlist(results))
}

# Searching for "punish" and its derivatives
found_sentences <-
  search_term_in_corpus(doc_texts = FULL_corp, search_term = "Indian")

# Output found sentences
print(found_sentences)
```

# [CAN_timetrend.R]{.underline}

-   CAN_Universe_Cases.csv needed to match the number of elements/entries as were in the KPCA. This was giving us issues initially.

-   The graphing categories had to match our new data.

-   The output was terrible (the legend was taking over the whole graph) so we blanked it out.

## Time Trend – PC1:

```{r time_trends}
# Analyze Time Trends:
# Universe of Cases (again, needs to match elements)
universe <- read.csv("./00_documents/01_CAN_spreadsheets/CAN_universe_cases_douglas.csv")
universed <- gsub("\\/", "-", universe$Date)
universe$Date <-  as.Date(universed, format = "%Y-%m-%d")


# Pull in PC1 estimates
load(file = "./00_scripts/01_CAN/CAN_KPC1_douglas.rdata")
rot <- KPC1@rotated

require(RPMG)
repca <- RESCALE(
  rot,
  nx1 = 0,
  nx2 = 1,
  minx = min(rot),
  maxx = max(rot)
)

bd <- data.frame(dates = universe$Date,
                 rpca = as.numeric(repca),
                 types = universe$Type)

bdo <- bd[order(bd[, 1]),]

# do trend test; assume data is monthly

require(tis)
# wrong start date, but doesn't matter for current purposes
ts.dat <- tis(bdo[, 2], start = c(2000, 1), freq = 12)

# Do formal test:
# non-parametric Spearman test between the observations and time
require(pastecs)
ttdat <- trend.test(ts.dat)

# get breakpoints
require(strucchange)
bp.ind <- breakpoints(ts.dat ~ 1)
# pull and report best fitting model
best.fit <- bdo[bp.ind$breakpoints, c(1, 2)]
# unique(bdo$types)

# plot of time series
# Define a named vector for the color mapping
color_map <- c(
  "Peace and Friendship" = "green",
  "Treaties of Peace and Neutrality" = "red",
  "Upper Canada Land Surrenders" = "purple",
  "Douglas Treaties" = "orange",
  "Robinson Treaties" = "cyan",
  "Royal Proclamation" = "pink",
  "Williams Treaties" = "yellow1",
  "The Numbered Treaties: 1" = "blue",
  "The Numbered Treaties: 2" = "blue",
  "The Numbered Treaties: 3" = "blue",
  "The Numbered Treaties: 4" = "blue",
  "The Numbered Treaties: 5" = "blue",
  "The Numbered Treaties: 6" = "blue",
  "The Numbered Treaties: 7" = "blue",
  "The Numbered Treaties: 8" = "blue",
  "The Numbered Treaties: 9" = "blue",
  "The Numbered Treaties: 10" = "blue",
  "The Numbered Treaties: 11" = "blue"
)

# Map the colors to the types in bdo
cols <- color_map[as.character(bdo$types)]

# Define a named vector for the shape mapping
shape_map <- c(
  "Peace and Friendship" = 4,
  "Treaties of Peace and Neutrality" = 1,
  "Upper Canada Land Surrenders" = 25,
  "Douglas Treaties" = 1,
  "Robinson Treaties" = 2,
  "Royal Proclamation" = 1,
  "Williams Treaties" = 2,
  "The Numbered Treaties: 1" = 1,
  "The Numbered Treaties: 2" = 1,
  "The Numbered Treaties: 3" = 1,
  "The Numbered Treaties: 4" = 1,
  "The Numbered Treaties: 5" = 1,
  "The Numbered Treaties: 6" = 1,
  "The Numbered Treaties: 7" = 1,
  "The Numbered Treaties: 8" = 1,
  "The Numbered Treaties: 9" = 1,
  "The Numbered Treaties: 10" = 1,
  "The Numbered Treaties: 11" = 1
)

# Map the shapes to the types in bdo
shapes <- shape_map[as.character(bdo$types)]

# main plot
plot.it.out1 <- function()
{
  par(bg = 'cornsilk1')
  scaled <- bdo$rpca
  plot(
    bdo$dates,
    scaled,
    pch = as.numeric(shapes),
    type = "p",
    col = "black",
    bg = as.character(cols),
    cex = 1.5,
    lwd = .5
  )
  abline(v = as.Date("1871-01-01"),
         lty = 3,
         ylab = "")
  
  lines(bdo$dates, bdo$rpca, col = "gray50")
  points(
    bdo$dates,
    bdo$rpca,
    pch = as.numeric(shapes),
    type = "p",
    col = "black",
    bg = as.character(cols),
    cex = 1.5
  )
  
  # put spline on
  xx <- lowess(bdo$dates, bdo$rpca)$x
  yy <- lowess(bdo$dates, bdo$rpca)$y
  
  lines(xx, yy, lwd = 2)
  
  # Commenting out legend:
  #  legend(
  #    "topright",
  #    pch = c(21, 24, 22),
  #    pt.bg = c("green", "red", "purple", "orange","cyan", "yellow1", "blue"),
  #    col = "black",
  #    legend = c("P & F", "UCLS", "Douglas", "Robinson", "Williams", "NT1", "NT2", "NT3", "NT4",
  #               "NT5", "NT6", "NT7", "NT8", "NT9", "NT10", "NT11"),
  #    pt.cex = 1.5
  #  )
  
  # detrend the rotated pca scores
  res <- resid(lm(bdo[, 2] ~ seq(bdo[, 2])))
  # plot(bdo[,1], res, type="l")
}

# plot with change points
plot.break <- function()
{
  par(bg = 'cornsilk1')
  scaled <- bdo$rpca
  plot(
    bdo$dates,
    scaled,
    pch = as.numeric(shapes),
    type = "p",
    col = "black",
    bg = as.character(cols),
    cex = 1.5,
    lwd = .5
  )
  
  lines(bdo$dates, bdo$rpca, col = "gray90")
  points(
    bdo$dates,
    bdo$rpca,
    pch = as.numeric(shapes),
    type = "p",
    col = "gray50",
    bg = "gray90",
    cex = 1.5
  )
  # break points
  abline(v = best.fit[, 1],
         lwd = 4,
         col = "black")
  
  # get segment lowess
  bps <- bp.ind$breakpoints
  bpts <- c(1, bps, nrow(bdo))
  for (i in 1:(length(bpts) - 1))
  {
    y1 <-
      lowess(bdo$dates[bpts[i]:bpts[i + 1]], bdo[bpts[i]:bpts[i + 1], 2])$y
    x1 <-
      lowess(bdo$dates[bpts[i]:bpts[i + 1]], bdo[bpts[i]:bpts[i + 1], 2])$x
    lines(x1, y1, lwd = 2, col = "red")
    text(
      x = mean(bdo$dates[bpts[i]:bpts[i + 1]]),
      y = 0.0,
      label = round(mean(bdo[bpts[i]:bpts[i + 1], 2]), d = 3),
      cex = 1.5
    )
  }
  
  
  
}

# cols <- sapply(bdo$types, function(t)

# plot residualized values
plot.res <- function()
{
  par(bg = 'cornsilk1')
  res <- resid(lm(bdo[, 2] ~ seq(bdo[, 2])))
  plot(bdo[, 1], res, ylab = "residuals", type = "n")
  
  # lines for pre1871 trs
  lines(bdo[bdo[, 1] < "1871-01-01", 1], res[bdo[, 1] < "1871-01-01"], col =
          "darkgreen")
  
  # lines for post1871 trs
  lines(bdo[bdo[, 1] > "1871-01-01", 1], res[bdo[, 1] > "1871-01-01"], col =
          "blue")
  
  abline(lm(res ~ bdo[, 1]), col = "red", lwd = 3)
  abline(v = as.Date("1871-06-06"), lty = 3)
}

cat("\n plot.it.out1() to see main graphic\n\n")
```

### Ugly Graph Output – PC1:

```{r ugly_graph_output_pc1}
plot.it.out1()
```

### Graph – KPC1:

```{r graph_kpc1}
require(RPMG)
# universe of cases (again, needs to match elements)
universe <-
  read.csv("./00_documents/01_CAN_spreadsheets/CAN_universe_cases_douglas.csv")

# Clean Dates
universed <- gsub("\\/", "-", universe$Date)
universe$Date <-  as.Date(universed, format = "%Y-%m-%d")

# Adjust name to "The Numbered Treaties"
# Regular expression to match "The Numbered Treaties:" followed by any characters.
pattern <- "The Numbered Treaties:.*"

universe$Type <- gsub(pattern = pattern, "The Numbered Treaties", universe$Type)


# Pull in PC1 estimates
load(file = "./00_scripts/01_CAN/CAN_KPC1_douglas.rdata")
rot <- KPC1@rotated

repca <- RESCALE(
  rot,
  nx1 = 0,
  nx2 = 1,
  minx = min(rot),
  maxx = max(rot)
)

bd <- data.frame(dates = universe$Date,
                 rpca = as.numeric(repca),
                 types = universe$Type)

bdo <- bd[order(bd[, 1]), ]

# do trend test; assume data is monthly

require(tis)
# wrong start date, but doesn't matter for current purposes
ts.dat <- tis(bdo[, 2], start = c(2000, 1), freq = 12)

# Do formal test:
# non-parametric Spearman test between the observations and time
require(pastecs)
ttdat <- trend.test(ts.dat)

# get breakpoints
require(strucchange)
bp.ind <- breakpoints(ts.dat ~ 1)
# pull and report best fitting model
best.fit <- bdo[bp.ind$breakpoints, c(1, 2)]

mrg <- list(l = 100, r = 100, b = 100, t = 100, pad = 100)

plot_ly(data = bdo, x = ~ dates, y = ~ rpca, type = 'scatter', mode = 'markers', color = ~ types) %>%
  layout(
    title = "<b>Time-Series</b>\nFirst Principal Component", 
    xaxis = list(
      # Format to display only the year
      tickformat = "%Y", type = "date", title = "Year"),
    yaxis = list(title = "PC1 (KPCA)"), 
    margin = mrg) 
```

## Time Trend – PC2:

```{r time_trend_pc2}
# Analyze Time Trends:
# universe of cases (again, needs to match number of elements)
universe <-
  read.csv("./00_documents/01_CAN_spreadsheets/CAN_universe_cases_douglas.csv")
universed <- gsub("\\/", "-", universe$Date)
universe$Date <-  as.Date(universed, format = "%Y-%m-%d")


# Pull in PC2 estimates
load(file = "./00_scripts/01_CAN/CAN_KPC2_douglas.rdata")
rot <- KPC2@rotated

require(RPMG)
repca <- RESCALE(
  rot,
  nx1 = 0,
  nx2 = 1,
  minx = min(rot),
  maxx = max(rot)
)

bd <- data.frame(dates = universe$Date,
                 rpca = as.numeric(repca),
                 types = universe$Type)

bdo <- bd[order(bd[, 1]), ]

# do trend test; assume data is monthly

require(tis)
# wrong start date, but doesn't matter for current purposes
ts.dat <- tis(bdo[, 2], start = c(2000, 1), freq = 12)

# Do formal test:
# non-parametric Spearman test between the observations and time
require(pastecs)
ttdat <- trend.test(ts.dat)

# get breakpoints
require(strucchange)
bp.ind <- breakpoints(ts.dat ~ 1)
# pull and report best fitting model
best.fit <- bdo[bp.ind$breakpoints, c(1, 2)]
# unique(bdo$types)

# plot of time series
# plot of time series
# Define a named vector for the color mapping
color_map <- c(
  "Peace and Friendship" = "green",
  "Treaties of Peace and Neutrality" = "red",
  "Upper Canada Land Surrenders" = "purple",
  "Douglas Treaties" = "orange",
  "Robinson Treaties" = "cyan",
  "Royal Proclamation" = "pink",
  "Williams Treaties" = "yellow1",
  "The Numbered Treaties: 1" = "blue",
  "The Numbered Treaties: 2" = "blue",
  "The Numbered Treaties: 3" = "blue",
  "The Numbered Treaties: 4" = "blue",
  "The Numbered Treaties: 5" = "blue",
  "The Numbered Treaties: 6" = "blue",
  "The Numbered Treaties: 7" = "blue",
  "The Numbered Treaties: 8" = "blue",
  "The Numbered Treaties: 9" = "blue",
  "The Numbered Treaties: 10" = "blue",
  "The Numbered Treaties: 11" = "blue"
)

# Map the colors to the types in bdo
cols <- color_map[as.character(bdo$types)]

# Define a named vector for the shape mapping
shape_map <- c(
  "Peace and Friendship" = 4,
  "Treaties of Peace and Neutrality" = 1,
  "Upper Canada Land Surrenders" = 25,
  "Douglas Treaties" = 1,
  "Robinson Treaties" = 2,
  "Royal Proclamation" = 1,
  "Williams Treaties" = 2,
  "The Numbered Treaties: 1" = 1,
  "The Numbered Treaties: 2" = 1,
  "The Numbered Treaties: 3" = 1,
  "The Numbered Treaties: 4" = 1,
  "The Numbered Treaties: 5" = 1,
  "The Numbered Treaties: 6" = 1,
  "The Numbered Treaties: 7" = 1,
  "The Numbered Treaties: 8" = 1,
  "The Numbered Treaties: 9" = 1,
  "The Numbered Treaties: 10" = 1,
  "The Numbered Treaties: 11" = 1
)

# Map the shapes to the types in bdo
shapes <- shape_map[as.character(bdo$types)]

# main plot
plot.it.out2 <- function()
{
  par(bg = 'cornsilk1')
  scaled <- bdo$rpca
  plot(
    bdo$dates,
    scaled,
    pch = as.numeric(shapes),
    type = "p",
    col = "black",
    bg = as.character(cols),
    cex = 1.5,
    lwd = .5
  )
  abline(v = as.Date("1871-01-01"),
         lty = 3,
         ylab = "")
  
  lines(bdo$dates, bdo$rpca, col = "gray50")
  points(
    bdo$dates,
    bdo$rpca,
    pch = as.numeric(shapes),
    type = "p",
    col = "black",
    bg = as.character(cols),
    cex = 1.5
  )
  
  # put spline on
  xx <- lowess(bdo$dates, bdo$rpca)$x
  yy <- lowess(bdo$dates, bdo$rpca)$y
  
  lines(xx, yy, lwd = 2)
  
  # Commenting out legend:
  #  legend(
  #    "topright",
  #    pch = c(21, 24, 22),
  #    pt.bg = c("green", "red", "purple", "orange","cyan", "pink", "yellow1", "blue"),
  #    col = "black",
  #    legend = c("P & F", "UCLS", "Douglas", "Robinson", "Royal Proclamation", Williams", "NT1", "NT2", "NT3", "NT4",
  #               "NT5", "NT6", "NT7", "NT8", "NT9", "NT10", "NT11"),
  #    pt.cex = 1.5
  #  )
  
  # detrend the rotated pca scores
  res <- resid(lm(bdo[, 2] ~ seq(bdo[, 2])))
  # plot(bdo[,1], res, type="l")
}


# plot with change points
plot.break <- function()
{
  par(bg = 'cornsilk1')
  scaled <- bdo$rpca
  plot(
    bdo$dates,
    scaled,
    pch = as.numeric(shapes),
    type = "p",
    col = "black",
    bg = as.character(cols),
    cex = 1.5,
    lwd = .5
  )
  
  lines(bdo$dates, bdo$rpca, col = "gray90")
  points(
    bdo$dates,
    bdo$rpca,
    pch = as.numeric(shapes),
    type = "p",
    col = "gray50",
    bg = "gray90",
    cex = 1.5
  )
  # break points
  abline(v = best.fit[, 1],
         lwd = 4,
         col = "black")
  
  # get segment lowess
  bps <- bp.ind$breakpoints
  bpts <- c(1, bps, nrow(bdo))
  for (i in 1:(length(bpts) - 1))
  {
    y1 <-
      lowess(bdo$dates[bpts[i]:bpts[i + 1]], bdo[bpts[i]:bpts[i + 1], 2])$y
    x1 <-
      lowess(bdo$dates[bpts[i]:bpts[i + 1]], bdo[bpts[i]:bpts[i + 1], 2])$x
    lines(x1, y1, lwd = 2, col = "red")
    text(
      x = mean(bdo$dates[bpts[i]:bpts[i + 1]]),
      y = 0.0,
      label = round(mean(bdo[bpts[i]:bpts[i + 1], 2]), d = 3),
      cex = 1.5
    )
  }
  
  
  
}

# cols <- sapply(bdo$types, function(t)

# plot residualized values
plot.res <- function()
{
  par(bg = 'cornsilk1')
  res <- resid(lm(bdo[, 2] ~ seq(bdo[, 2])))
  plot(bdo[, 1], res, ylab = "residuals", type = "n")
  
  # lines for pre1871 trs
  lines(bdo[bdo[, 1] < "1871-01-01", 1], res[bdo[, 1] < "1871-01-01"], col =
          "darkgreen")
  
  # lines for post1871 trs
  lines(bdo[bdo[, 1] > "1871-01-01", 1], res[bdo[, 1] > "1871-01-01"], col =
          "blue")
  
  abline(lm(res ~ bdo[, 1]), col = "red", lwd = 3)
  abline(v = as.Date("1871-06-06"), lty = 3)
}

cat("\n plot.it.out2() to see main graphic\n\n")
```

### Ugly Graph Output – PC2:

```{r ugly_graph_pc2}
plot.it.out2()
```

### Graph – PC2:

```{r visualize_pc2}
require(RPMG)
# universe of cases (again, needs to match elements)
universe <-
  read.csv("./00_documents/01_CAN_spreadsheets/CAN_universe_cases_douglas.csv")

# Clean Dates
universed <- gsub("\\/", "-", universe$Date)
universe$Date <-  as.Date(universed, format = "%Y-%m-%d")

# change The Numbered Treaties: 11 -> The Numbered Treaties
# Regular expression to match "The Numbered Treaties:" followed by any characters
pattern <- "The Numbered Treaties:.*"

universe$Type <- gsub(pattern = pattern, "The Numbered Treaties", universe$Type)


#pull in pca estimates
load(file = "./00_scripts/01_CAN/CAN_KPC2_douglas.rdata")
rot <- KPC2@rotated

repca <- RESCALE(
  rot,
  nx1 = 0,
  nx2 = 1,
  minx = min(rot),
  maxx = max(rot)
)

bd <- data.frame(dates = universe$Date,
                 rpca = as.numeric(repca),
                 types = universe$Type)

bdo <- bd[order(bd[, 1]), ]

# do trend test; assume data is monthly

require(tis)
# wrong start date, but doesn't matter for current purposes
ts.dat <- tis(bdo[, 2], start = c(2000, 1), freq = 12)

# Do formal test:
# non-parametric Spearman test between the observations and time
require(pastecs)
ttdat <- trend.test(ts.dat)

# get breakpoints
require(strucchange)
bp.ind <- breakpoints(ts.dat ~ 1)
# pull and report best fitting model
best.fit <- bdo[bp.ind$breakpoints, c(1, 2)]

mrg <- list(l = 100, r = 100, b = 100, t = 100, pad = 100)

plot_ly(data = bdo, x = ~ dates, y = ~ rpca, type = 'scatter', mode = 'markers', color = ~ types) %>%
  layout(
    title = "<b>Time-Series</b>\nSecond Principal Component", 
    xaxis = list(
      # Format to display only the year
      tickformat = "%Y", type = "date", title = "Year"),
    yaxis = list(title = "PC2 (KPCA)"), 
    margin = mrg)
```

# Spirling's Original Code

### kpcastuff01.R

-   Try UTF-8-sig encoding, or Linux/Mac.

```{r}
# estimate 1d kpca of treaties
# rm(list = ls())

# load package
# library(tm)

# m <- Sys.time()

# treatiesV <- Corpus(
#   DirSource("c:/treaties/nonames/justdocsVCUT"),
#   readerControl = list(
#     reader = readPlain,
#     language = "en",
#     load = F
#   )
# )

# treatiesA <- Corpus(
#   DirSource("c:/treaties/nonames/justdocsACUT"),
#   readerControl = list(
#     reader = readPlain,
#     language = "en",
#     load = F
#   )
# )

# treatiesR <- Corpus(
#   DirSource("c:/treaties/nonames/justdocsRCUT"),
#   readerControl = list(
#     reader = readPlain,
#     language = "en",
#     load = F
#   )
# )

# treatiesU <- Corpus(
#   DirSource("c:/treaties/nonames/justdocsUCUT"),
#   readerControl = list(
#     reader = readPlain,
#     language = "en",
#     load = F
#   )
# )

# set.seed(2)

# sampV <-
#   treatiesV # only those signed BEFORE 1871 -- not including "z" class etc.
# sampA <-
#   treatiesA[1:77] # post 1871 treaties (but drop last one -- from 1954): think of treating as ending in 1914
# sampR <- treatiesR # all rejected--incl post 71
# sampU <- treatiesU # as it happens, all signed before 1871

# corp <- c(sampV, sampA, sampR, sampU)

# require(kernlab)

# stringkern <- stringdot(type = "string", length = 5)

# stringpcaAJPSrep <- kpca(
#   corp,
#   kernel = stringkern,
#   kpar = list(sigma = 0.1),
#   features = 1,
#   th = 1e-4,
#   na.action = na.omit
# )

# save(stringpcaAJPSrep, file = "./00_data/treaties_USA/")
# save(stringpcaAJPSrep, file = "c:/treaties/nonames/AJPSreplicationfiles/pca01ajpsrep.rdata")
```

### timetrend.R

```{r}
#########################
## analyze time trends ##
## vis-a-vis  treaties ##
#########################

# rm(list = ls())


# universe of cases
# universe <-read.csv("./00_data/treaties_USA/UniverseCases.csv")

# universed <- gsub("\\/", "-", universe$Date)
# universe$Date <-  as.Date(universed, format = "%m-%d-%Y")

# pull in pca estimates
# load(file = "c:/projects/treaties/nonames/AJPSreplicationfiles/pca01ajpsrep.rdata")
# rot <- stringpcaAJPSrep@rotated

# require(RPMG)
# repca <- RESCALE(
#   rot,
#   nx1 = 0,
#   nx2 = 1,
#   minx = min(rot),
#   maxx = max(rot)
# )

# bind with date
# bd <-
#   data.frame(dates = universe$Date,
#              rpca = as.numeric(repca),
#              types = universe$Type)
# bdo <- bd[order(bd[, 1]), ]

# do trend test
# assume data is monthly (over 595 months!)

# require(tis)
# wrong start date, but doesn't matter
# for current purposes
# ts.dat <- tis(bdo[, 2], start = c(2000, 1), freq = 12)

# do formal test
# non-parametric Spearman test
# between the observations and time
# require(pastecs)
# ttdat <- trend.test(ts.dat)

# get breakpoints
# require(strucchange)
# bp.ind <- breakpoints(ts.dat ~ 1)
# pull and report best fitting model
# best.fit <- bdo[bp.ind$breakpoints, c(1, 2)]


# plot of time series
# cols <- as.character(bdo$types)
# cols[which(bdo$types %in% c("Operable", "Ratified Agreement"))] <-
#   "green"
# cols[which(bdo$types %in% c("Rejected by Congress"))] <- "red"
# cols[which(bdo$types %in% c("Unratified treaty"))] <- "purple"

# shapes <- as.character(bdo$types)
# shapes[which(bdo$types %in% c("Operable", "Ratified Agreement"))] <-
#   21
# shapes[which(bdo$types %in% c("Rejected by Congress"))] <- 24
# shapes[which(bdo$types %in% c("Unratified treaty"))] <- 22

# main plot
# plot.it.out <- function()
# {
#   par(bg = 'cornsilk1')
#   scaled <- bdo$rpca
#   plot(
#     bdo$dates,
#     scaled,
#     pch = as.numeric(shapes),
#     type = "p",
#     col = "black",
#     bg = as.character(cols),
#     cex = 1.5,
#     lwd = .5
#   )
#   abline(v = as.Date("1871-01-01"),
#          lty = 3,
#          ylab = "")
 
#   lines(bdo$dates, bdo$rpca, col = "gray50")
#   points(
#     bdo$dates,
#     bdo$rpca,
#     pch = as.numeric(shapes),
#     type = "p",
#     col = "black",
#     bg = as.character(cols),
#     cex = 1.5
#   )
  
  # put spline on
#   xx <- lowess(bdo$dates, bdo$rpca)$x
#   yy <- lowess(bdo$dates, bdo$rpca)$y
  
#   lines(xx, yy, lwd = 2)
  
#   legend(
#     "topright",
#     pch = c(21, 24, 22),
#     pt.bg = c("green", "red", "purple"),
#     col = "black",
#     legend = c("valid, ratified",
#                "rejected", "unratified"),
#     pt.cex = 1.5
#   )
  
  
  # detrend the rotated pca scores
#   res <- resid(lm(bdo[, 2] ~ seq(bdo[, 2])))
  # plot(bdo[,1], res, type="l")
# }


# plot with change points
# plot.break <- function()
# {
#   par(bg = 'cornsilk1')
#   scaled <- bdo$rpca
#   plot(
#     bdo$dates,
#     scaled,
#     pch = as.numeric(shapes),
#     type = "p",
#     col = "black",
#     bg = as.character(cols),
#     cex = 1.5,
#     lwd = .5
#   )
  
#   lines(bdo$dates, bdo$rpca, col = "gray90")
#   points(
#     bdo$dates,
#     bdo$rpca,
#     pch = as.numeric(shapes),
#     type = "p",
#     col = "gray50",
#     bg = "gray90",
#     cex = 1.5
#   )
  # break points
#   abline(v = best.fit[, 1],
#          lwd = 4,
#          col = "black")
  
  # get segment lowess
#   bps <- bp.ind$breakpoints
#   bpts <- c(1, bps, nrow(bdo))
#   for (i in 1:(length(bpts) - 1))
#   {
#     y1 <-
#       lowess(bdo$dates[bpts[i]:bpts[i + 1]], bdo[bpts[i]:bpts[i + 1], 2])$y
#     x1 <-
#       lowess(bdo$dates[bpts[i]:bpts[i + 1]], bdo[bpts[i]:bpts[i + 1], 2])$x
#     lines(x1, y1, lwd = 2, col = "red")
#     text(
#       x = mean(bdo$dates[bpts[i]:bpts[i + 1]]),
#       y = 0.0,
#       label = round(mean(bdo[bpts[i]:bpts[i + 1], 2]), d = 3),
#       cex = 1.5
#     )
#   }
  
  
  
# }


# plot residualized values
# plot.res <- function()
# {
#   par(bg = 'cornsilk1')
#   res <- resid(lm(bdo[, 2] ~ seq(bdo[, 2])))
#   plot(bdo[, 1], res, ylab = "residuals", type = "n")
  
  # lines for pre1871 trs
#   lines(bdo[bdo[, 1] < "1871-01-01", 1], res[bdo[, 1] < "1871-01-01"], col =
#           "darkgreen")
  
  # lines for post1871 trs
#   lines(bdo[bdo[, 1] > "1871-01-01", 1], res[bdo[, 1] > "1871-01-01"], col =
#           "blue")
  
#   abline(lm(res ~ bdo[, 1]), col = "red", lwd = 3)
#   abline(v = as.Date("1871-06-06"), lty = 3)
# }

# cat("\n plot.it.out() to see main graphic\n\n")
```
